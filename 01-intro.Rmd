# Introduction {#intro}

Who was the last person who fully understood all areas of biology and medicine? As the fields have grown, it has become impossible for one researcher or doctor to keep track of the latest research across such broad fields. This question is a popular discussion amongst mathematicians as there are several arguable candidates for the last mathematician who truly understood all the branches of the field at their time. Famous minds, like Euler or Gauss, are commonly cited. Some of the most remarkable work in recent mathematics, such as Andrew Wile’s proof of Fermat’s Last Theorem [@wiles1995modular], has required the use of multiple branches of mathematics. Uniting knowledge from diverse areas biology will become essential to solving applied medical problems [@altman2018challenges,@national2014convergence]. 

Getting the right research to the right researchers is a major bottleneck. The topics in the field go from the micro-scale of protein interactions and genetic modifications to the macro-scale of clinical trials and healthcare systems. An individual researcher needs to know more about different areas of biology in order to plan out an experiment and interpret the results. This problem is complicated by the increasing rate of publications in biomedicine [@lu2011pubmed]. These problems necessitate automated text mining methods to help digest and disseminate research results.

The primary driving forces for text mining development are challenges in research communication which are illustrated by three anecdotes. First, Gregor Mendel's seminal genetics work on pea plants was published in 1866 [@mendel1866versuche]. It lay dormant, acquiring a small number of citations over the following thirty years until being rediscovered in the early 20th century. This indicates the importance of the venue used for publication and clarity of language. Could a cure for an important disease have already been published but not been recognized by appropriate researchers? The second anecdote describes the fear that a huge problem in one field is equivalent to a solved problem in another field. Temple F. Smith and Michael S. Waterman having published the now famous Smith-Waterman algorithm for local sequence alignment [@smith1981comparison] discovered the similar problem of aligning stratigraphic sequences in geology. This discovery came through serendipity as the two researchers walked through a geology department and saw a research poster visualizing the alignment problem [@smithvanbug] and not because either of them was involved in geology research. They were quickly able to publish a paper using a similar algorithm for this problem [@smith1980new]. The third anecdote describes a case where valuable information is mentioned as a small section of a paper. As it is not a key result of the paper, it is not mentioned in the abstract and overlooked by most researchers. The discovery and understanding of green fluorescent protein (GFP) is an example of this phenomenon. The bioluminescent properties of a protein are discussed only as a footnote in the paper describing the purification of the aequorin protein from the Aequorea jellyfish [@shimomura1962extraction]. These three anecdotes cannot be isolated incidents. There will be numerous further cases of “undiscovered public knowledge” (a term popularised by Dr. Don Swanson [@swanson1986undiscovered]) where the solution to a research or clinical question already exists within published literature.

Text mining research and the larger natural language processing (NLP) research area use computers to understand human-written text and provide new ways for humans to interact with digital media. Text mining processes large corpora of text for particular types of knowledge that can both direct users towards relevant knowledge and structure the knowledge for easy assimilation. Researchers should use text mining methods in everyday use to collate relevant knowledge and stay up-to-date. The goal of this work is to understand the problems impeding this goal and solve several of them.

One area in which this need to combine knowledge from the macro to the micro is the area of personalized cancer treatment, also known as precision oncology. This approach aims to use genome sequencing data of individual patient tumors to guide clinical decision making. By identifying the genetic mistakes that are causing the uncontrolled cell growth of a tumor and integrating knowledge from across biology, clinicians will be able to understand the reasons behind a cancer’s development and hopefully find weaknesses that can be targeted. The knowledge for this is contained within basic biological research studies of protein function and cell biology, larger sequencing studies and statistical analysis, clinical trial data, clinical guidelines and pharmacological recommendations and many other sources of knowledge. Most of this knowledge is contained within published academic literature which is indexed in PubMed.

This thesis develops and carefully evaluates approaches to applying text mining technology for extracting and inferring biomedical knowledge from published literature. We turn these methods to problems faced in personalized cancer treatment research in order to create valuable knowledge bases that condense tens of thousands of papers for easier survey. This combined work moves us closer to a research world where scientists work with text mining tools in order to keep up-to-date with distilled knowledge relevant to their research. 

## Background
 
The following sections will outline the current status of research into biomedical text mining and the relevant problems faced in personalized cancer medicine.

### Biomedical text mining

Text mining is the application of informatics to process text documents to retrieve or extract information [@ananiadou2006text]. In the biomedical field, this can focus on text from published literature, electronic health records, clinical guidelines and any other text source that contains knowledge about medicine or biology. The field broadly focuses on two main applications, information retrieval (IR) for identifying relevant documents and information extraction (IE) for siphoning relevant knowledge in a structured fashion.

In order to extract and structure knowledge from published literature on a large scale, computers must be able to process the raw text. However, computers are designed to deal with numerical data. Text data does not translate well into a form for easy computation. It is stored as a list of characters, either ASCII, Unicode or another encoding. Computers cannot glean any level of understanding from the raw bytes of a sentence. Various steps need to happen in order to build structure from this raw data. These approaches are common in all natural language processing (NLP) solutions and are not specific to the biomedical domain.
 
The first challenge is generally to split the series of characters into sentences. In most writing, a period is a good predictor of the end of a sentence and rules can be used to catch exceptions. Exceptions include acronyms and titles (such as U.K. and Dr.). These sentences are then split into tokens, generally individual words, which can be treated as independent elements. These tokens can be further processed to identify the part-of-speech (e.g. noun), remove stems (e.g. -ing) and other lemmatization methods (e.g. plurals -> singular). These further steps depend on the downstream analysis to be performed on the data. Statistical systems have been built that integrate knowledge to combine these steps together such as the Stanford CoreNLP parser [@manning2014stanford]. These parsers can then identify substructures within sentences such as noun or verb phrases. Furthermore, additional structure such as dependency parses can build information about how different tokens within a sentence relate to each other (e.g. a noun may be a subject of a verb).
 
### Information Retrieval
 
Information retrieval (IR) is the task of finding and prioritizing relevant documents for a particular search task. Researchers use these methods daily by searching for academic papers using tools such as PubMed and Google Scholar. Advances aim to improve relevance for search results. A single researcher has a practical limit of the number of papers that they can read in one year, so it is of paramount concern how they select those papers. IR methods can be used to search other text corpora such as clinical guidelines, but the largest research focus is on academic paper retrieval.

Biomedical IR work has benefitted from the approaches developed for web search. Most methods require a set of keywords as input and then return a prioritized list of papers. Older web search tools, such as Altavista, used direct keyword matching and simple heuristics based on the frequency [@lawrence2000accessibility]. Search tools encouraged website developers to add hidden metadata into the header information of a web page. Both these methods placed significant trust in the content developers to provide relevant information. Google’s Pagerank method dramatically changed how search results were prioritized [@brin1998anatomy]. By treating the web links as a graph, they could model the “importance” of certain websites by the number of websites linking to it. 
 
In the biomedical domain, similar challenges existed for search. Many journals required (and still require) authors to provide keywords for their paper. This data could be used to help indexing and searching papers but were not associated with a standardized ontology. This created inconsistency. In order to solve this, the National Library of Medicine (NLM) developed the Medical Subject Headings ontology (MeSH). This is used to manually annotate all citations in NLM's PubMed indexing service by highly skilled annotators. With this information, PubMed’s search can return highly relevant papers for a provided topic. Advanced search functionality allows control of the journals to search, years, authors and many other factors. 

For a long time, their search facility ordered results by reverse chronological order. Recent advances have introduced a relevance ranking method that uses different factors including publication type, year and data on how the search term matches the document [@fiorini2017cutting]. A Pagerank-like approach is more challenging in academic literature as the only links between papers are citations which are not truly analogous to links between web pages. PubMed recently implemented a relevance rank system that combined various data types to improve the relevance of the search results [@fiorini2018best].

A similar IR problem to search is the identification of similar documents. In this case, the input is a current document (either published or free text) and the output is a prioritized list of published works that are similar. This document similarity metric is a feature of PubMed through their “Similar articles” option. One solution to this problem uses ideas in document clustering. The basic concept is to group documents that discuss similar topics. The route to extract the topics discussed in a paper can be quite varied. For biomedical abstracts, the associated MeSH terms provide a rich and high quality manually curated resource to allow for document clustering. Simple overlap metrics based on MeSH terms can provide good quality results for similar document classification [@zhu2009enhancing]. The textual content of the document can be interrogated directly. The simplest method groups documents that share similar words. This data is often very sparse as the English vocabulary is very large and similar ideas can be expressed using very different words. Preprocessing methods that standardize case, turn plurals in singulars and other steps can reduce the sparsity. Term normalization can be used to group different synonyms together that describe the same term. Each document is represented by a numeric vector. This is either counts of associated metadata terms or counts of words within the document. This numeric count data is known as a “bag-of-words”. To find similar documents, these vectors are then compared often with Euclidean distance or cosine distance.

A popular document clustering method designed for this problem is Latent Semantic Analysis (LSA) [@deerwester1990indexing] which treats document clustering as an unsupervised learning problem, specifically as a learning by compression problem. It transforms the text documents in a word frequency matrix where documents are along one axis and each word in the vocabulary is along the other axis. Every occurrence of a word $i$ in a document $j$ increments the value of $x_{ij}$. Hence most of the matrix will be zero. It uses low-rank singular value decomposition (SVD) to compress the sparse data into a small dense space where similar topics will be represented by similar latent variables. One other way to detect similarity between papers is by looking at the similarity of their citations. Papers that cite similar papers, or at least papers with similar topics themselves, likely have similar topics. However, citation networks are challenging to build due to paper and author ambiguity and duplicates [@carpenter2014challenges]. 

Document classification can be invaluable for problems in information retrieval. It uses the content and potentially metadata of a document to predict the specific topic of the document. Similar to document clustering methods it uses word frequencies within the document represented as sparse count vectors. However, as a supervised method, it requires sample documents that have been annotated with specific classes (e.g. the topic of the paper, or whether the document is of interest to the researcher). A traditional binary classifier then attempts to identify the words that make the most accurate predictions. In the biomedical space, there is particular interest in predicting the MeSH terms for a paper to assist in the laborious task undertaken by the National Library of Medicine to annotate all biomedical abstracts with terms from the MeSH ontology. Given the huge number of existing annotated abstracts as training data, several methods have been developed for this task as part of a regular competition, BioASQ [@tsatsaronis2015overview].

### Information Extraction
 
Information extraction (IE) methods identify structured information from a span of text, an entire document or even a large corpus of documents. This allows text to be transformed into a standardized format that can be easily searched, queried and processed by other algorithms. These methods are valuable in the biomedical field for extracting knowledge from published literature, automating the analysis of electronic medical records and many other applications. There are three main problems that information extraction methods try to solve: coreference resolution, named entity recognition and relation extraction.
 
Coreference resolution addresses the problem of anaphora. Pronouns and non-specific terms are frequently used to refer back to entities named in previous sentences (e.g. “he was first prescribed the drug in 2007”). Coreference resolution attempts to link these terms to their original citation. This can be challenging as there can be many candidate coreferences for a single pronoun in a sentence. For example, the word “it” in a sentence could refer to any of the previous objects mentioned in a document. A naive approach would simply use the most recent noun but this is often wrong. Context must be used to infer which coreferences are most likely [@soon2001machine]. Furthermore, by processing all coreference decisions at the same time, more optimal solutions can be found that don’t create contradictions where the same person is both the subject and object of an inconsistent action (e.g. “she passed her the newspaper’) [@clark2015entity].

Named entity recognition (NER) identifies mentions of specific entities such as genes and drugs. Basic approaches can use exact string matching with a list of entity names (e.g. synonyms of genes provided by the UMLS metathesaurus [@bodenreider2004unified]). NER methods can make use of context within a sentence to predict tokens that would likely be a certain entity type. For instance, a token that comes before “expression” and is all capitals, e.g. “EGFR expression” is likely a gene. NER methods often make use of approaches based on Hidden Markov Models (HMM) or Continuous Random Fields (CRF). These are finite-state based methods that can assign labels to tokens in a sequence provided a set of training data. Exact string matching can provide very high recall but with lower precision due to high levels of ambiguity for frequently used English words (e.g. “ICE” is a gene name, but is frequently “ice” in non-gene contexts). HMM/CRF methods will provide better precision as they can take the context into account but requires a good training set for the associated entity type. Entity normalization approaches take a tagged entity in a sentence and connect it back to an ontology using the context and a set of synonyms associated with each ontology item. Successful NER tools include BANNER [@leaman2008banner] for many entity types, DNorm [@leaman2013dnorm] for diseases and tmChem [@leaman2015tmchem] for chemicals.

Relation extraction predicts whether a relation exists between two or more entities provided with text in which these entities appear. These methods may also try to differentiate the type of relationship between these terms (e.g. whether a drug treats or causes a disease). The most basic approach to identify whether a relationship exists between two entities is the use of co-occurrences. At its most basic, this method states that a relation exists between entities if they ever appear within a span of text. The text length can vary depending on the application, but sentences and abstracts are common. This binary decision will lead to very high recall of relations but also likely a high false positive rate. 

There are alternative metrics than the simple binary decision of whether a co-occurrence ever appears. Intuitively two terms that appear together in many sentences are more likely to be part of a relationship. When taken across a large corpus of documents, e.g. all publications in a journal or even all accessible biomedical literature, the frequency of co-occurrences can be very high. However, for a single document, these methods may not be applicable. A threshold can be used to cut off co-occurrences that appear too infrequently. These infrequent co-occurrences may be false positives. However, a small number may be valuable information that are simply not commonly discussed. 

Co-occurrences will be affected by the frequency of the individual terms. Frequently mentioned terms, such as “breast cancer”, will have higher co-occurrence numbers than rarely discussed terms such as “ghost cell carcinoma”. Hence a normalization approach that takes into account the background frequency of individual terms can help identify spurious co-occurrences driven by the fact that one or the other term occurs a lot. “Breast cancer” appears in many papers and so is more likely to cooccur with terms. By taking the frequency of the words “breast cancer” into account, we can reduce the false positives. At the same time, we can put greater importance on the few co-occurrences of terms with “ghost cell carcinoma”. This concept is used in the term-frequency inverse-document-frequency (TF-IDF) approach to normalization. Term frequency is the count of terms and inverse document frequency is the normalizer for the frequency of the term in general. 
 
The power of co-occurrences really comes from aggregated information across a large corpus. For individual documents, more advanced relation extraction methods can be used. These can take for the form of supervised approaches (which require substantial example text data), semi-supervised approaches (which require less example data and is easier to acquire) or unsupervised approaches (which use no prior knowledge).
 
Supervised learning approaches to relation extraction involve a training set of text with annotated entities and relations. The general goal is to transform the text and annotations into a form amenable to traditional classification methods. A common method is to vectorize the candidate relation within a sentence so that it is represented by a numerical (often sparse and very large) vector that can be fed into a standard binary classifier (e.g. logistic regression or support vector machine). These methods use bag-of-words approaches similar to the document clustering discussed previously. This transforms the sentence into a vector representation of word counts. Bi-grams, tri-grams (or n-grams to generalize) capture neighboring two, three, or more words. They can also transform subsections of the sentence, e.g. the clause that contains the relation, or a window of words around each entity. The entity types can also be represented with one-hot vectors (where the vector is as long as the number of entity types with a value of one at the location corresponding to the entity type and zeroes elsewhere). These methods produce very sparse and large vectors and often p >> n, where p is the number of features and n is the number of examples used for training. These vectors can then be processed by classifiers such as logistic regression, support vector machines or random forests.
 
Support vector machines offer an alternative method that avoiding vectorizing the relations. A support vector machine attempts to find the hyperplane that separates the training examples. However, the power of SVMs really comes down to the “kernel trick” which allows SVMs to be solved by using comparisons between training examples instead of vectorizing them and placing them in N-dimensional space. A kernel is simply a similarity function that takes in two examples and returns a similarity value. Without a complex kernel, an SVM is known as a linear SVM and behaves very similarly to logistic regression. Popular kernels include polynomial functions and radial basis functions (RBF). These kernels implicitly transform the example data into another space where a separating hyperplane is easier to find. For text mining purposes, support vector machines are valuable for the ability of kernel functions to accept example data which aren’t numerical vectors. A string comparison kernel can accept two text strings as input and output a similarity measure based on metrics such as Hamming distance or edit distance. This means that a classifier can be built using a similarity measure and no vectorization is required. Furthermore, support vector machines do not require each input example to be compared with every single training example. The SVM identifies the training examples (known as the support vectors) that can be used to define the separating hyperplane. When applied to test data, comparisons are only needed against these “support vector” examples, which allows for a high-performance classifier.
 
With the advent of dependency parsing, relation extraction methods began to make use of the information. Bunescu and Mooney specifically argue that the main information about the relationship is contained within the dependency path which is the shortest path between two entities within the dependency parse tree [@bunescu2005shortest]. Kernels that used this information such as the dependency path kernel allows comparison of the dependency parse instead of the full sentence. These use a simple similarity metric based on the number of shared words, parts of speech, and entity types at each place within the two dependency paths being compared.
 
Deep learning methods have made great headway into non-biomedical information extraction problems with the main computational linguistics research venues being dominated by deep learning methods. These methods exploit the concept of distributional semantics. This is the idea that individual words can be represented as numerical vectors where similar words will have similar vectors. The bag-of-words approach to word representation does not fit this as each word is represented by a one-hot vector which is as a wide as the vocabulary and only has a single one. Each word is therefore orthogonal to all other words in the vocabulary. These techniques depend on large amounts of annotated data as the model complexity of deep learning is very high and methods are liable to overfit. Due to lack of data, deep learning has had a hard time gaining traction in biomedical text mining research.
 
Event extraction is a special type of relation extraction, sometimes denoted as complex relation extraction. It extracts events described in a sentence which may involve multiple relations. These relations have other relations as arguments instead of entities. There are three relations in this example sentence: "upregulation of one gene decreases phosphorylation of another protein". The upregulation would be one relation, the phosphorylation would be the second relation, and the decrease would be a compound relation connecting the other two relations. Event extraction has been the focus of several shared tasks such as GENIA [@kim2003genia]. The standard approach involves breaking the task down into a series of binary relation extractions which can be built up into a full event [@bjorne2015tees].

When fully annotated training data is not available, there are two possible options. Semi-supervised methods use partially annotated data or so-called silver-annotated data. This silver-annotated data is generated using a procedure known as distant supervision [@mintz2009distant]. When no annotations exist, existing knowledge bases which contain some relevant relations can be used to automatically annotate sentences. For instance, if erlotinib is known to inhibit EGFR, then all sentences which contain both terms could be annotated with this relation. This will produce a larger number of false positive annotations. But if there are enough “seed facts” in the knowledge base, a well-trained classifier may be able to identify the key patterns that link all the sentences and reduce the false positive rate. A fully unsupervised method based on clustering can also be used to group potential relations that look similar. Percha et al grouped relations based on their dependency path and then used a distant-supervision like approach to tag different relation clusters [@percha2018global].

All of these relation extraction methods will annotate a span of text with the location of the relationship and the entities associated with them. Depending on the application, these annotated documents could then be presented to the user, or the relations could be aggregated to allow easier searching. In order to drive research in relation extraction and other areas of biomedical information extraction, there are regular shared tasks organized by the research community. These are competitions where one group releases an annotated training set for other groups to build machine learning systems for. A held-out test set is then used to evaluate the competing algorithms. These competitions have included the BioNLP Shared Tasks [@kim2011overview, @kim2009overview], BioCreative tasks [@hirschman2005overview] and many others. They provide a good metric of the latest algorithms in the field. They are especially valuable as biomedical information extraction is hampered by the small annotation sets (compared to non-biomedical domains). Biomedical annotation often requires expert level knowledge and can be difficult to organize. These events encourage the development of methods that can work with few examples.

The documents for these shared tasks are often based on PubMed abstracts and full-text articles from PubMed Central. These resources, which are often used for text mining, are the easiest to access which is a common limiting factor in biomedical text mining. In contrast, it is very difficult to get access to a large corpus of electronic health records which limits the research opportunities in this area. In biomedicine, abstracts are easily accessible through PubMed and can be downloaded in bulk through the NCBI’s FTP service. However full-text articles are often challenging to access. The PubMed Open Access Subset provides full-text articles in XML format for over a million full-text articles. This is, however, a fraction of the publications in PubMed. Other researchers have tried mass downloading of the PDFs of published literature. Publishers often limited this in their terms of use contracts and have been known to limit access to their resources for entire universities to encourage individual researchers to desist from mass downloading [@bohannon2016s]. Even with a large set of PDFs, the conversion to processible text is incredibly challenging. PDF is a format designed for standardized viewing and printing across platforms and is not structured for easy extraction of text. Many tools have been developed to try to make this task easier [@ramakrishnan2012layout]. But with different journal formats, even simple tasks such as linking paragraphs across pages and removing page numbers are challenging.


### Knowledge Bases and Knowledge Graphs

Information extraction methods provide a means to extract relations between different entities.  By applying these methods to a well-defined problem and using large biomedical text as the input corpus, a variety of knowledge bases have been constructed. These include the STRING database which use co-occurrence methods to identify likely protein-protein interactions [@szklarczyk2014string]. The PubTator resource provides automatically annotated PubMed abstracts which are valuable for advanced searching and further text mining efforts [@wei2013pubtator]. An example of information extraction for a very specific domain is the miRTex database which collates information on microRNA targets [@li2015mirtex]. 

The relations within knowledge bases are often represented as triples. These triples are two entities and the relation that connects them. The set of triples can, therefore, be viewed as a directed graph where vertices are entities and directed labeled edges are relations. Knowledge bases that contain triples can then be queried using SPARQL [@prud2006sparql]. This is a database query language based on the structured query language (SQL) format used in normal relational databases. The key improvements of SPARQL are the ease of ability to query multiple databases (known as endpoints) and connect together diverse data sets (assuming they can be linked by appropriate unique identifiers).

A growing area of research is inference on knowledge bases. This can involve asking questions of the knowledge base by traversing the knowledge base [@athenikos2010biomedical]. It can also involve making predictions of additions to the knowledge base, particularly new edges to the knowledge base. Most knowledge inference work has focussed on non-biomedical knowledge graphs such as Freebase [@bollacker2008freebase]. The TransE [@bordes2013translating] and RESCAL [@nickel2012factorizing] methods focussed on the problem of knowledge base completion (KBC) where there are known to be edges missing. By using different latent-based approaches, they are able to prioritize missing edges. Several knowledge graphs have been built for biomedical knowledge either through manual curation or automated methods. The WikiData knowledge graph is the structured data backend for all of Wikipedia [@vrandevcic2014wikidata]. It contains a large amount of biological data that is mostly manually curated [@burgstaller2016wikidata] and provides a SPARQL endpoint for querying. Other knowledge graphs include KnowLife [@ernst2014knowlife] and GNPR [@percha2018global] which are extracted from text.

### Personalized Cancer Genomics

Cancer is a disease of uncontrolled cell growth caused by genomic abnormalities. These abnormalities include small point mutations, copy number variation, structural rearrangements, and epigenetic changes. These affect regulation of growth signaling, control of apoptosis, angiogenesis and many other factors that together are known as the hallmarks of cancer [@hanahan2000hallmarks]. These abnormalities can be caused by exogenous mutagens such as smoking or UV radiation, or endogenous mutagens such as oxidation and deamination. Certain chemotherapies can also be mutagenic as damaging DNA can prove lethal to the fast-dividing tumor cells. With the advances in sequencing technology, genomic interrogation of cancers has become commonplace. These investigations are confounded by the driver/passenger mutation paradigm which states that only a small fraction of genomic abnormalities are actually involved in the development of a cancer. These abnormalities (known as drivers) can inactivate key protective genes, or overactivate other genes that normally required careful regulation. The other abnormalities (known as passengers) do not have an oncogenic effect and have “come along for the ride” [@haber2007cancer]. 

The goal of personalized (or precision) medicine is to provide a treatment plan that is tailored to an individual patient. This idea holds great promise in cancer treatment as every patient’s cancer is different. No two cancers contain the exact same set of genomic abnormalities. By sequencing an individual tumor, researchers hope to identify which genomic aberrations are driver events to understand which pathways are essential to the growth of a cancer. Using this information, combined with knowledge of pharmacogenomics, individualized treatments can be identified.

The Personalized Oncogenomics (POG) project, based at the BC Cancer Agency, began in 2008. Through whole genome sequencing (WGS) and transcriptome sequencing (RNAseq), the genome and transcriptome are analyzed. Over time, the costs of sequencing have reduced dramatically [@weymann2017cost]. However, the cost of informatics and genome interpretation have remained stable. This is mostly due to the laborious and manual steps involved in understanding the relevance of important genomic abnormalities within the sequencing data. 

There are limited databases that provide some context on whether a likely mutation is a driver or passenger [@forbes2014cosmic] and how to clinically interpret variants [@tamborero2018cancer]. Much of this data is derived from the genomic survey provided by the Cancer Genome Atlas project [@weinstein2013cancer]. This means that analysts must search the vast biomedical literature to understand the latest research for many genes and variants. This area would benefit greatly from the development of new text mining approaches and resources to collate information on the relevance of genes and variants to different cancer types.

## Objectives and Chapter Overviews

The overall objectives of this thesis are to develop generalizable methods for extracting and inferring knowledge directly from published biomedical literature and apply these methods to problems faced by precision oncology. This work will move us one step closer to a world in which researchers use text mining tools and results in their everyday research. We provide a new method for literature-based knowledge discovery, a reliable approach to extract knowledge from sentences applicable to many other biology problems and finally text mined knowledge bases that the precision oncology community can easily access.

Chapter 2 proposes a method for building knowledge graphs using co-occurrences and inferring new knowledge that will likely appear in future publications. By building a dataset of biomedical co-occurrences from the PubMed and PubMed Central Open Access datasets, we are able to construct a knowledge graph using publications up to the year 2010. A test set is then constructed using publications after 2010 and different prediction methods are compared against it. A comparison of our matrix decomposition method with the other leading solutions to this knowledge inference problem shows that our approach gives dramatically improved performance and provide a step towards automated hypothesis generation for biologists.

Chapter 3 moves past co-occurrences as the method for extracting knowledge and towards full relation extraction based on a supervised learning approach. As part of the BioNLP 2016 Shared Task, we developed a generalizable relation extraction method that builds features from the sentence containing a candidate relation and uses support vector machines. This tool, known as VERSE, went on to win the Bacteria Biotope subtask, came third in the Seed Development subtask and outperformed deep learning based methods. The chapter includes our further development of generalizable relation extraction tools with the Kindred Python package that integrates with many other biomedical text mining platforms including PubTator [@wei2013pubtator] and PubAnnotation [@kim2012pubannotation].

Chapter 4 begins to look at applying the information extraction methods to problems faced in personalized cancer treatment. In order to automate the analysis of individual patient tumors, a knowledge base of known drivers, oncogenes, and tumor suppressors is absolutely essential. In order to understand the purpose of a particular genomic aberration, the role of the associated gene must be known for the cancer. Unfortunately, this has previously required manual searching of literature. In this chapter, we describe the development of the CancerMine resource using a supervised learning approach. We hypothesized that the necessary information for drivers, oncogenes and tumor suppressors would be contained within single sentences and that our previously developed methods could be used to extract this information en masse from published literature. To this end, a team of annotators has curated a set of sentences related to the roles of different genes in cancer. By using the methods developed in Chapter 3, we build a machine learning pipeline that can efficiently process the entire biomedical literature and extract cancer gene roles. This data is kept up-to-date and is available to the precision cancer community for easy searching. This data can be integrated into precision oncology pipelines to flag genomic aberrations that are within relevant genes for that cancer type. The annotated set of sentences is also available to the text mining community as a dataset on which to evaluate future relation extraction methods.

Chapter 5 advances our knowledge of clinically relevant biomarkers in cancer. The Clinical Interpretation of Variants in Cancer (CIViC) database is a community-curated knowledge base for diagnostic, prognostic, predisposing and drug resistance biomarkers in cancer [@griffith2017civic]. This information is invaluable in automating a precision oncology analysis and providing actionable information to clinicians. In order to identify gaps in the CIViC knowledge and prioritize biomarkers that should be curated, we identify published sentences that likely contain all the relevant information. A team of eight curators worked to annotate sentences to link cancers, genes, drugs, and variants as biomarkers. This complex dataset is used to develop a multi-stage extraction system. We provide further advances with a ternary relation extraction system to integrate drug information. Through validation by the CIViC curation team, we illustrate the power of this methodology for extracting high-quality complex biological knowledge in bulk. This approach is able to provide a vast dataset of very high quality and can easily be applied to other problems in biology and medicine. Furthermore, the dataset of cancer biomarkers is valuable to all groups curating knowledge in precision medicine and also all analysts that are interrogating the genomes of patient tumors.

Finally, Chapter 6 concludes the thesis and discusses the successes and limitations of the research approaches taken. It explores interesting future directions that could be taken with the generalized and high performing methods developed in this thesis and with the valuable precision oncology datasets extracted from the literature.
