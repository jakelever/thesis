# Introduction {#intro}
 
A common discussion amongst mathematicians is the identify of the last mathematician who truly understood all the branches of the field at the time. Famous minds, like Euler or Gauss, are commonly cited as researchers who worked across many different areas and were able to bring them together to solve diverse problems. As mathematics has expanded, it has become impossible to even know the basics of all areas let alone fully understand them. In recent years, famous proofs, such as Andrew Wile’s proof of Fermat’s Last Theorem [@wiles1995modular], has required the use of multiple branches of mathematics.

This question has a poignancy in biology and medicine as the field continues to expand. Can any biologist claim to have a full understanding of all the areas of biology that are currently being researched? The topics in the field go from the micro-scale of protein interactions anda genetic modifications to the macro-scale of clinical trials and health care systems. Recent discussions about the advances in data science have argued that the next generation of translational researchers will need to combine knowledge from all scales of biological and medical research [@altman2018challenges, @national2014convergence]. The lowering costs of sequencing technologies and other investigatory techniques (e.g. mass spectrometry) provide more sources of data that can be combined with the growing datasets of phenotype data such as the UK biobank and the AllOfUs initiative [@sudlow2015uk].

All together, this means that an individual researcher needs to know more about different areas of biology in order to plan out an experiment and interpret the results. This problem is complicated by the increasing rate of publications in biomedicine as documented by analysis of PubMed [@lu2011pubmed]. Research dissemination is going to become a larger problem in the future. This task requires communicating relevant research results to the small number of individuals in the world who would really benefit from the knowledge. Review papers are a key method to collate valuable research information. An interesting alternative to a review paper has been proposed in the machine learning field. The concept of research distillation, especially through interactive journal formats, is a new area where the latest research is explained in a visually appealing and intuitive way. This movement argues against the research hazing of other researchers with the time cost of reading all the background papers in order to understand the latest research [cite ResearchDebt distil].

This leads to the greatest fear that a cure for an important disease has already been discovered but the relevance is not understood. Alexander Fleming’s work is cited as an example of this fear. His discovery of penicillin in 1929 was published in the British Journal of Experimental Pathology [@fleming1929antibacterial]. But the importance of penicillin wasn’t realized until years later. In 1940 researchers at Oxford identified the therapeutic potential of penicillin and it was quickly produced on an industrial scale for the war effort [@howie1986penicillin]. This missing decade creates questions of the effects on patients lives and the challenges of research dissemination. Another fear of researchers is that a huge problem in their field is equivalent to a solved problem in another field. Temple F. Smith and Michael S. Waterman having published the now famous Smith-Waterman algorithm for local sequence alignment [@smith1981comparison] discovered the similar problem of aligning stratigraphie sequences in geology. This discovery came through serendipity as the two researchers walked through a geology department and saw a research poster visualising the alignment problem [cite Vanbug talk] and not because either of them were involved in geology research. They were quickly able to publish a paper using a similar algorithm for this problem [@smith1980new]. There are also well documented cases where valuable information is mentioned as a small section of a paper and, as it is not a key result of the paper, are not mentioned in the abstract and are overlooked by most researchers. The discovery and understanding of green fluorescent protein (GFP) is an example of this phenomenon. The bioluminescent properties of a protein are discussed only as a footnote in the paper describing the purification of the aequorin protein from the Aequorea jellyfish [@shimomura1962extraction]. These three anecdotes cannot be isolated incidents. There will be numerous further cases of “undiscovered public knowledge” (a term popularised by Dr. Don Swanson [@swanson1986undiscovered]) where the solution to a research or clinical question already exists within published literature.

One area in which this need to combine knowledge from the macro to the micro is the area of personalized cancer treatment, also known as precision oncology. This approach aims to use genome sequencing data of individual patient tumors to guide clinical decision making. By identifying the genetic mistakes that are causing the uncontrolled cell growth of a tumor and integrating knowledge from across biology, clinicians will be able to understand the reasons behind a cancer’s development and hopefully find weaknesses that can be targeted. The knowledge for this is contained within basic biological research studies of protein function and cell biology, larger sequencing studies and statistical analysis, clinical trial data, clinical guidelines and pharmacological recommendations and many other sources of knowledge. Most of this knowledge is contained within published academic literature which is indexed in PubMed.

This information overload points towards the need for informatics help to digest the important research. Text mining research and the larger natural language processing (NLP) research community aim to use computers to understand human-written text and provide new ways for humans to interact with digital media. Text mining in particular aims to mine large corpora of text for particular types of knowledge that can both direct users towards relevant knowledge and structured the knowledge for other algorithmic analysis.

This thesis develops and carefully evaluates approaches to apply text mining technology for extracting and inferring biomedical knowledge from published literature We turn these methods to problems faced in personalized cancer treatment research in order to create valuable knowledge bases that condense tens of thousands of papers to easier perusal. This combined work moves us closer to a research world where scientists work with text mining tools in order to be kept up-to-date with distilled knowledge relevant to their research. 

## Background
 
The following sections will outline the current status of research into biomedical text mining and the relevant problems faced in personalized cancer medicine.

### Biomedical text mining

Text mining is the application of informatics to process text documents to retrieve or extract information [cite and quote something if you want]. In the biomedical field, this can focus on text from published literature, electronic health records, clinical guidelines and any other text source that contains knowledge about medicine or biology. The field broadly solves two main applications, information retrieval (IR) for identifying relevant documents and information extraction (IE) for PULLING relevant knowledge in a structured fashion.

In order to extract and structure knowledge from published literature in a large scale, computers must be able to process the raw text. However, computers are designed to deal with numerical data. Text data does not translate well into a form for easy computation. It can be represented as a list of characters, either ASCII, Unicode or another encoding. Computers cannot glean any level of understanding from the raw bytes of a sentence. Various steps need to happen in order to build structure from this raw data. These approaches are common in all natural language processing (NLP) solutions and are not specific to the biomedical domain.
 
The first challenge is generally to split the series of characters into sentences. Regular expressions were used for this problem for a long time. In most writing, a period is a good predictor of the end of a sentence and rules can be used to catch exceptions. Exceptions include acronyms and titles (such as Dr.). These sentences are then split into tokens, generally individual words, which can be treated as independent elements. These tokens can be further processed to identify the part-of-speech (e.g. noun), remove stems (e.g. -ing) and other lemmatization methods (e.g. plurals -> singular). These further steps depend on the downstream analysis to be performed on the data. Statistical systems have been built that integrate knowledge to combine these steps together such as the Stanford CoreNLP parser. These parsers can then identify substructures within sentences such as noun or verb phrases. Furthermore additional structure such as dependency parses can build information about how different tokens within a sentence relate to each other (e.g. a noun may be a subject of a verb).
 
### Information Retrieval
 
Information retrieval (IR) is the task of finding and prioritising relevant documents for a particular search task. Researchers with use these methods daily by searching for academic papers using tools such as PubMed and Google Scholar. Advances in this field try to develop search results with better relevance. A single researcher has a practical limit of the number of papers that they can read in one year, so it is of paramount concern how they select those papers. IR methods can be used to search other text corpora such as clinical guidelines, but the largest research focus is on academic paper retrieval.
 
Most search methods require a set of keywords as input and then return a prioritized list of papers. Search research was driven forward for the growth of the World Wide Web. Older search tools, such as Altavista, used direct keyword matching and simple heuristics based on the frequency [@lawrence2000accessibility]. Search tools encouraged website developers to add hidden metadata into the header information of a web page. Both these methods placed significant trust in the content developers to provide relevant information to. Nefarious web content creators who wanted to direct web traffic towards their pages in order to profit from advertising revenue would add huge sets of keywords to the main content and metadata of the page. This increased the likelihood of any search term returning this webpage as a highly relevant page. Google’s Pagerank method dramatically changed how search results were prioritised [@brin1998anatomy]. By treating the web links as a graph, they could model the “importance” of certain websites by the number of websites linking to it. This meant that a website such as the BBC would be deemed higher important and any website that they linked to would also raise in importance. This information would then be used in ranking the search results.
 
Different indexing systems exist for academic work. In biology, the most popular is PubMed, the indexing service provided by the National Library of Medicine (NLM). Some journals require authors to provide a set of keywords. But these can be inconsistent and are not linked to a standardized nomenclature or ontology. Instead NLM created the MeSH annotation system. This involves highly skilled annotators annotating a paper with a set of terms that denote the topics of the paper, such as genes or drugs discussed as main topics in the paper. With this information, PubMed’s search can return highly relevant papers for a provided topic. Advanced search functionality allows TIGHT control of the journals to search, years, authors and many other. For a long time, their search facility ordered results by reverse chronological order. Recent advances have introduced a relevance ranking method that uses different factors including publication type, year and data on how the search term matches the document [@fiorini2017cutting]. A Pagerank-like approach is more challenging in academic literature as the only links between papers are citations which are not truly analogous to links between webpages. One reason is that the resulting graph should be a tree as no cycles should exist. Any traversal of a citation graph should always lead towards older papers. This makes it harder to prioritise important papers except by the simple citation count metric which biases older papers in more popular areas of research.
 
A similar IR problem to search is identification of similar documents. In this case, the input is a current document (either published or free text) and the output is a prioritised list of published works that are similar. This document similarity metric is a feature of PubMed as each citation has an option for “Similar articles”. One solution to this problem uses ideas in document clustering. The basic concept is to group documents that discuss similar topics. The route to extract the topics discussed in a paper can be quite varied. For biomedical abstracts, the associated MeSH terms provide a rich and high quality manually curated resource to allow for document clustering. Simple overlap metrics based on MeSH terms can provide good quality results for similar document classification [cite]. The textual content of the document can be interrogated directly. The simplest method groups documents that share similar words. This data is often very sparse as the English vocabulary is very large and similar ideas can be expressed using very different words. Preprocessing methods that standardize case, turn plurals in singulars and other steps can reduce the sparsity. Term normalization can be used to group different synonyms together that really describe the same term. Each document is represented by a numeric vector. This is either counts of associated metadata terms, counts of words within the document, or this data represented in a compressed latent space. This numeric count data is known as a “bag-of-words”. To find similar documents, these vectors are then compared often with Euclidean distance or cosine distance.

A popular document clustering method designed for this problem is Latent Semantic Analysis (LSA) [@deerwester1990indexing] which treats document clustering as an unsupervised learning problem, specifically as a learning by compression problem. It transform the text documents in a word frequency matrix where documents are along one axis and each word in the vocabulary is along the other axis. Every occurrence of a word i in a document j increments the value of x_ij. Hence most of the matrix will be zero. It uses low-rank singular value decomposition (SVD) to compress the sparse data into a small dense space where similar topics will be represented by similar latent variables. One other way to detect similarity between papers is by looking at the similarity of their citations. Papers that cite similar papers, or at least papers with similar topics themselves, likely have similar topics. However citation networks are challenging to build due to paper and author ambiguity [cite]. 
 
Learning by compression is the underlying concept of other matrix decomposition methods including alternating least squares (ALS) and non-negative matrix factorization (NMF). These two methods compress a large, generally sparse, matrix into a smaller denser matrix. ALS puts no constraints on the smaller matrices, in contrast to SVD which requires linear independence between rows and columns in the decomposed matrices. NMF is similar to ALS with the requirement that all values in the dense matrices are non-negative. Deep learning methods are also based on learning by compression concepts. Autoencoders offer a non-linear method for data compression into a latent space which has shown great promise in computer vision [cite].
 
Document classification, another problem in information retrieval, uses the content and potentially metadata of a document to predict the specific topic of the document. Similar to document clustering methods it uses word frequencies within the document represented as sparse count vectors. However, as a supervised method, it requires sample douments that have been annotated with specific classes (e.g. the topic of the paper, or whether the document is of interest to the researcher). A traditional binary classifier then attempts to identify the words that make the most accurate predictions. In the biomedical space, there is particular interest in predicting the MeSH terms for a paper to assist in the laborious task undertaken by the National Library of Medicine to annotate all biomedical abstracts with terms from the MeSH ontology. Given the huge number of existing annotated abstracts as training data, several methods have been developed for this task as part of a regular competition, BioASQ [@tsatsaronis2015overview].

### Information Extraction
 
Information extraction (IE) methods identify structured information from a span of text, an entire document or even a large corpus of documents. This allows text to be transformed into a standardized format that can be easily searched, queried and processed by other algorithms. These methods are valuable in the biomedical field for extracting knowledge from published literature, automating analysis of electronic medical records and many other applications. There are three main problems that information extraction methods try to solve: coreference resolution, named entity recognition and relation extraction.
 
Coreference resolution addresses the problem of anaphora. Pronouns and non-specific terms are frequently used to refer back to entities named in previous sentences (e.g. “he was first prescribed the drug in 2007”). Coreference resolution attempts to link these terms to their original citation. This can be challenging as there can be many candidate coreferences for a single pronoun in a sentence. For example, the word “she” in a sentence could refer to any of the previous women mentioned in a document. A naive approach would simply used the most recent noun but this is often wrong. Context must be used to infer which coreferences are most likely [@soon2001machine]. Furthermore by processing all coreference decisions at the same time, more optimal solutions can be found that don’t create contradictions where a person is both the subject and object of an inconsistent action (e.g. “she passed her the newspaper’) [@clark2015entity].

Named entity recognition (NER) identifies mentions of specific entities such as genes and drugs. Basic approaches can use exact string matching with a list of entity names (e.g. synonyms of genes provided by the UMLS metathesaurus [@bodenreider2004unified]). NER methods can make use of context within a sentence to predict tokens that would likely be a certain entity type. For instance, a token that comes before “expression” and is all capitals, e.g. “EGFR expression” is likely a gene. NER methods often make use of approaches based on Hidden Markov Models (HMM) or Continuous Random Fields (CRF). These are finite-state based methods that can assign labels to tokens in a sequence provided a set of training data. Exact string matching can provide very high recall but with lower precision due to high levels of ambiguity for frequently used English words (e.g. “ICE” is a gene name, but is frequently a “ice” in non-gene contexts). HMM/CRF methods will provide better precision as they can take the context into account but requires a good training set for the associated entity type. Entity normalization approaches take a tagged entity in a sentence and connect it back to an ontology using the context and a set of synonyms associated with each ontology item. Sometimes entity normalization is essential. For instance, a gene mention that cannot be connected to the HUGO gene nomenclature list may not be useful for further processing. Successful NER tools include BANNER [@leaman2008banner] for many entity types, DNorm [@leaman2013dnorm] for diseases and tmChem [@leaman2015tmchem] for chemicals.

Relation extraction predicts whether a relation exists between two or more entities provided with text in which these entities appear. These methods may also try to differentiate the type of relation between these terms (e.g. whether a drug treats or causes a disease). The most basic approach to identify whether a relation exists between two entities is the use of cooccurrences. At its most basic, this method states that a relation exists between entities if they ever appear within a span of text. The text length can vary depending on application, but sentences and abstracts are common.

This binary decision will lead to very high recall of relations but also likely a high false positive rate. There are alternative metrics than the simple binary decision of whether a cooccurrence ever appears. Intuitively two terms that appear together in many sentences are more likely to be part of a relationship. When taken across a large corpus of documents, e.g. all publications in a journal or even all accessible biomedical literature, the frequency of cooccurrences can be very high (example). However for a single document, these methods may not be applicable. A threshold can be used to cut off cooccurrences that appear too infrequently.
 
These infrequent cooccurrences may be false positives. However a small number may be valuable information that is simply not commonly discussed. Cooccurrences will be affected by the frequency of the individual terms. Frequently mentioned terms, such as “breast cancer”, will have higher cooccurrence numbers than rarely discussed terms such as “ghost cell carcinoma”. Hence a normalization approach that takes into account the background frequency of individual terms can help identify spurious cooccurrences driven by the fact that one or the other terms occurs a lot. “Breast cancer” appears in many papers and so is more likely to cooccur with terms. By taking the frequency of the words “breast cancer” into account, we can reduce the false positives. At the same time, we can put greater importance on the few cooccurrences of terms with “ghost cell carcinoma”. This concept is used in the term-frequency inverse-document-frequency (TF-IDF) approach to normalization. Term frequency is the count of terms and inverse document frequency is the normalizer for the frequency of the term in general. 
 
The power of coccurrences really comes from aggregated information across a large corpus. For individual documents, more advanced relation extraction methods can be used. These can take for the form of supervised approaches (which require substantial example text data), semi-supervised approaches (which require less example data and is easier to acquire), unsupervised approaches (which use no prior knowledge).
 
Supervised learning approaches to relation extraction involve a training set of text with annotated entities and relations. The general goal is to transform the text and annotations into a form amenable for traditional classification methods. A common method is to vectorize the candidate relation within a sentence so that it is represent by a numerical (often sparse and very large) vector that can be fed into a standard binary classifier (e.g. logistic regression or support vector machine). These methods use bag-of-words approaches similar to the document clustering discussed previously. This transforms the sentence into a vector representation of word counts. Bi-grams, tri-grams (or n-grams to generalise) capture neighbouring two, three, or more words. They can also transform subsections of the sentence, e.g. the clause that contains the relation, or a window of words around each entity. The entity types can also be represented with one-hot vectors (where the vector is as long as the number of entity types with a value of one at the location corresponding to the entity type and zero elsewhere). These methods produce very sparse and large vectors and often p >> n , where p is the number of features and n is the number of examples used for training. These vectors can then be processed by classifiers such as logistic regression, support vector machines or random forests.
 
Support vector machines offer an alternative method that avoiding vectorizing the relations. A support vector machine attempts to find the hyperplane that separates the training examples. However the power of SVMs really comes down to the “kernel trick” which allows SVMs to be solved by using comparisons between training examples instead of vectorizing them and placing them in N-dimensional space. A kernel is simply a similarity function that takes in two examples and returns a similarity value. Without a complex kernel, an SVM is known as a linear SVM and behaves very similarly to logistic regression. Popular kernels include polynomial functions and radial basis functions (RBF). These kernels implicitly transform the example data into another space where a separating hyperplane is easier to find. For text mining purposes, support vector machines are valuable for the ability of kernel functions to accept example data which aren’t numerical vectors. A string comparison kernel can accept two text strings as input and output a similarity measure based on metrics such as Hamming distance or edit distance. This means that a classifier can be built using a similarity measure and no vectorization is required. Furthermore, support vector machines do not require each input example to be compared with every single training example. The SVM identifies the training examples (in this case known as the support vectors) that can be used to define the separating hyperplane. When applied to test data, comparisons are only needed against these “support vector” examples, which allows for a high performance classifier.
 
With the advent of dependency parsing, relation extraction methods began to make use of the information. Mooney et al specifically argue that the main information about the relation is contained within the dependency path which is the shortest path between two entities within the dependency parse tree [@bunescu2005shortest]. Kernels that used this information such as the dependency path kernel allows comparison of.the dependency parse instead of the full sentence. These use a simple similarity metric based on the number of shared words, parts of speech, and entity types at each place within the two dependency paths being compared.
 
Deep learning methods have made great headway into non-biomedical information extraction problems with the main computational linguistics conferences being dominated by deep learning methods. These methods exploit the concept of distributional semantics. This is the idea that individual words can be represented as numerical vectors where similar words will have similar vectors. The bag-of-words approach to word representation does not fit this as each word is represented by a one-hot vector which is as a wide as the vocabulary and only has a single one. Each word is therefore orthogonal to all other words in the vocabulary.
 
Distributional semantics has similar foundations to the latent semantic analysis approach to document clustering and representation. By enforcing a smaller representation than there are words in the vocabulary, learning by compression will group words together to use similar representations. These methods provide information about the frequent cooccurrences between words, The most frequently cited method, word2vec [@mikolov2013distributed], frames the problem as the following: Can a neural network predict whether a sequence of 5 words (a 5-gram) is likely? The positive training data are 5-grams extracted from a large corpora of text (e.g. newspaper data). The negative training data are perturbed 5-grams from the same corpora where one of the words has been changed to a random word. The words are represented by one-hot vectors. The hidden layers of the neural network learn a latent and dense representation of each word. This is the representation that can then be used in other text mining problems. An alternative to the word2vec problem is skip-grams which turns the problem around [@mikolov2013efficient]. Given a 5-gram with a missing word, the system is training to predict the missing word. This system will also learn representations for each word in the hidden layer. These word representations have the peculiar property of containing properties within geometric operations. The famous “king – man + woman = queen” example is often cited to show that not only are words represented in a dense numerical space, but also relationships (e.g. gender).
 
Most classifier (including classical neural networks) are limited by a fixed input and output. This made operating on sequences (e.g. text data) more challenging as sentences could vary a lot in length. This obstacle was removed with the development of recurrent neural networks [@goodfellow2016deep]. These systems allowed a one-to-one, one-to-many, many-to-one or many-to-many input/output patterns. Hence an input sentence of multiple tokens (many) could be used as input and one output could be generated (whether a sentence is giving a happy sentiment). Their ability to vary the size of input or output is due to the fact the information is retained within the hidden layer as a sequence is processed. This hidden layer becomes an input layer to the new hidden layer as the classifier moves onto the next element in the sequence. Different architectures have been tested out for this approach. Long-short-term memory (LSTM) networks have proven the most successful for many tasks and involves small modules that contain memory that can be passed onto modules used for the next token in the sequence. Bi-directional classifiers that scan a sequence in one direction and then in the reverse direction in order to provide long distance information from both views have also been shown to perform well in many tasks. The main shortcoming of deep learning methods is due to their model complexity, they require large amounts of data. However for some tasks, they can use unlabelled data to “jump-start” the values of weights within the hidden layer.
 
Event extraction is a special type of relation extraction, sometimes denoted as complex relation extraction. It extracts events described in a sentence which may involve multiple relations. These relations can have other relations as arguments instead of entities. For instance upregulation of one gene decreases phosphorylation of another protein. The upregulation would be one relation, the phosporation would be a second relation, and the decrease would be a compound relation connecting the other two relations. Event extraction has been the focus of several shared tasks such as GENIA [cite]. The standard approach involves breaking the task down into a series of binary relation extractions which can be built up into a full event [cite TEES].

When fully annotated training data is not available, there are two possible options. Semi-supervised methods use partially annotated data, or so-called silver-annotated data. This silver-annotated data is generated using a procedure known as distant supervision [@mintz2009distant]. When no annotations exist, existing knowledge bases which contain some relevant relations can be used to automatically annotate sentences. For instance, if erlotinib is known to inhibit EGFR, then all sentences which contain both terms could be annotated with this relation. This will produce a larger number of false positive annotations. But if there are enough “seed facts” in the knowledge base, a well-trained classifier may be able to identify the key patterns that link all the sentences and reduce the false positive rate. A fully unsupervised method based on clustering can also be used to group potential relations that look similar. Percha et al grouped relations based on their dependency path and then used a distant-supervision like approach to tag different relation clusters [@percha2018global].

All of these relation extraction methods will annotate a span of text with the location of the relation and the entities associated with them. Depending on the application, these annotated documents could then be presented to the user, or the relations could be aggregated to allow easier searching. In order to drive research in relation extraction and other areas of biomedical information extraction, there are regular shared tasks organised by the research community. These are competitions where one group releases an annotated training set for other groups to build machine learning systems for. A held-out test set is then used to evaluate the competing algorithms. These competitions have included the BioNLP Shared Tasks [cite], BioCreative tasks [cite], i2b2 and many others. They provide a good metric of the latest algorithms in the field. They are especially valuable as biomedical information extraction is hampered by the small annotation sets (compared to non-biomedical domains). Biomedical annotation often requires expert level knowledge and can be difficult to organise. These events show the success of methods trained on fewer examples.

The documents for these shared tasks are often based on Pubmed abstracts and full text articles from Pubmed Central. This is because these are often the target for information extraction problems, especially due to their ease of access which is a limiting factor for many text mining projects. As an example, it is generally very difficult to get access to electronic medical records unless the researcher is part of a hospital network. In biomedicine, abstracts are easily accessible through Pubmed and can be downloaded in bulk through the NCBI’s FTP service. However full text articles are often challenging to access. The Pubmed Open Access Subset provides full text articles in XML format for approximately. This is however a fraction of the publications in PubMed. Other researchers have tried mass downloading of the PDFs of published literature. Publishers often limited this in their terms of use contracts and have been known to limit access to their resources for entire universities to encourage individual researchers to desist from mass downloading [@bohannon2016s]. Even with a large set of PDFs, the conversion to processible text is incredibly challenging. PDF is a format designed for standardized viewing and printing across platforms and is not structured for easy extraction of text. Many tools have been developed to try to make this task easier [@ramakrishnan2012layout] . But with different journal formats, even simple tasks such as linking paragraphs across pages and removing page numbers are challenging.


### Knowledge Bases and Knowledge Graphs

Information extraction methods provide a means to extract relations between different entities.  By applying these methods to a well-defined problem and using large biomedical text as the input corpus, a variety of knowledge bases have been constructed. These include the STRING database which use cooccurrence methods to identify likely protein-protein interactions [@szklarczyk2014string]. The PubTator resource provides automatically annotated Pubmed abstracts which can be valuable for advanced searching and further text mining efforts [@wei2013pubtator]. An example of information extraction for a very specific domain is the miRTex database which collates information on microRNA targets [@li2015mirtex]. 

The relations within knowledge bases are often represented as triples. These triples are two entities and the relation that connects them. The set of triples can therefore be viewed as a directed graph where vertices are entities and directed labelled edges are relations. Knowledge bases that contain triples can then be queried using SPARQL [@prud2006sparql]. This is a database query language based on the structured query language (SQL) format used in normal relational databases. The key improvements of SPARQL are the ease of ability to query multiple databases (known as endpoints) and connect together diverse data sets (assuming they can be linked by appropriate unique identifiers).

A further area of research is inference on knowledge bases. This can involve asking questions of the knowledge base by traversing the knowledge base [@athenikos2010biomedical]. It can also involve making predictions of additions to the knowledge base, particularly new edges to the knowledge base. Most knowledge inference work has focussed on non-biomedical knowledge graphs such as Freebase [@bollacker2008freebase]. The TransE [@bordes2013translating] and RESCAL [@nickel2012factorizing] methods focussed on the problem of knowledge base completion (KBC) where there are known to be edges missing. By using different latent-based approaches, they are able to prioritise missing edges. Several knowledge graphs have been built for biomedical knowledge either through manual curation or automated methods. The WikiData knowledge graph is the structured data backend for all of Wikipedia [@vrandevcic2014wikidata]. It contains a large amount of biological data that is mostly manually curated [cite] and provides a SPARQL endpoint for querying. Other knowledge graphs include KnowLife [@ernst2014knowlife] and GNPR [@percha2018global] which are extracted from text.

### Personalized Cancer Genomics

Cancer is a disease of uncontrolled cell growth caused by genomic abnormalities. These abnormalities include small point mutations, copy number variation, structural rearrangements, and epigenetic changes. These affect regulation of growth signalling, control of apoptosis, angiogenesis and many other factors that together are known as the hallmarks of cancer [@hanahan2000hallmarks]. These abnormalities can be caused by exogenous mutagens such as smoking or UV radiation, or endogenous mutagens such as oxidation and deamination. Certain chemotherapies can also be mutagenic as damaging DNA can prove lethal to the fast-dividing tumor cells. With the advances in sequencing technology, genomic interrogation of cancers have become commonplace. These investigations are confounded by the driver/passenger mutation paradigm which states that only a small fraction of genomic abnormalities are actually involved in the development of a cancer. These abnormalities (known as drivers) can inactivate key protective genes, or activate other genes that normally required careful regulation. The other abnormalities (known as passengers) do not have an oncogenic effect and have “common along for the ride”. 

The goal of personalized (or precision) medicine is to provide a treatment plan that is tailored to an individual patient. This idea shows great promise in cancer treatment as every patient’s cancer is different. No two cancers contain the exact same set of genomic abnormalities. By sequencing an individual tumor, researchers hope to identify which genomic aberrations are driver events to understand which pathways are essential to the growth of a cancer. Using this information, combined with knowledge of pharmacogenomics, individualized treatments can be identified.

The Personalized Oncogenomics (POG) project, based at the BC Cancer Agency, began in 2008. Through whole genome sequencing (WGS) and transcriptome sequencing (RNAseq), the genome and transcriptome is analyzed. Over time, the costs of sequencing have reduced dramatically [@weymann2017cost]. However the cost of informatics and genome interpretation have remained stable. This is mostly due to the laborious and manual steps involved in identifying important genomic abnormalities within the sequencing data and associated clinical relevance. Analysts must search the vast biomedical literature to understand the latest research for each gene and variant. This is a problem that necessitates the development of new text mining approaches and resources.

 
## Objectives and Chapter Overviews

The overall objectives of this thesis are to develop methods for extracting and inferring knowledge directly from published biomedical literature and apply these methods to problems faced by precision oncology.

Chapter 2 proposes a method for building knowledge graphs using cooccurrences and inferring new knowledge that will likely appear in future publications. By building a dataset of biomedical cooccurrences from the PubMed and Pubmed Central Open Access datasets, we are able to construct a knowledge graph using publications up to the year 2010. A test set is then constructed  A comparison of our matrix decomposition method against the other leading solutions to this knowledge inference problem shows that “collaborative filtering” based methods give dramatically improved performance and provide a step towards automated hypothesis generation for biologists.

Chapter 3 moves past cooccurrences as the method for extracting knowledge and towards full relation extraction based on a supervised learning approach. As part of the BioNLP 2016 Shared Task, we develop a generalizable relation extraction method that builds features from the sentence containing a candidate relation and uses support vector machines. This tool, known as VERSE, went on to win the Bacteria Biotope subtask, came third in the Seed Development subtask and outperformed deep learning based methods. The chapter concludes with our further development of generalizable relation extraction tools with the Kindred Python package that integrates with many other biomedical text mining platforms including PubTator [@wei2013pubtator] and PubAnnotation [@kim2012pubannotation].

Chapter 4 begins to look at applying the information extraction methods to problems faced in personalized cancer treatment. In order to automate the analysis of individual patient tumors, a knowledge base of known drivers, oncogenes and tumor suppressors is absolutely essential. In order to understand the purpose of a particular genomic aberration, the role of the associated gene must be known for the cancer. Unfortunately this has previously required manual searching of literature. In this chapter, we describe the development of the CancerMine resource using a supervised learning approach. We hypothesized that the necessary information for drivers, oncogenes and tumor suppressors would be contained within single sentences and that our previously developed methods could be used to extract this information enmass from published literature. To this end, a team of annotators has curated a set of sentences related to the roles of different genes in cancer. By using the methods developed in Chapter 3, we build a machine learning pipeline that can efficiently process the entire biomedical literature and extract cancer gene roles. This data is kept up-to-date and is available to the precision cancer community for easier searching. This data can be integrated into precision oncology pipelines to flag genomic aberrations that are within relevant genes for that cancer type. The annotated set of sentences is also available to the text mining community as a dataset on which to evaluate future relation extraction methods.

Chapter 5 advances our knowledge on clinically relevant biomarkers in cancer. The CIViC database is a community-curated knowledge base for diagnostic, prognostic, predisposing and drug resistance biomarkers in cancer. This information is invaluable in automating a precision oncology analysis and providing actionable information to clinicians. In order to identify gaps in the CIViC knowledge and prioritise biomarkers that should be curated, we identify published sentences that likely contain all the relevant information. A team of eight curators worked to annotate sentences to link cancers, genes, drugs and variants as biomarkers. This complex dataset is used to develop a multi-stage extraction system. We provide further advances with a ternary relation extraction system to integrate drug information. Through validation by the CIViC curation team, we illustrate the power of this methodology for extracting high quality complex biological knowledge in bulk. This approach is able to provide a vast dataset of very high quality and can easily be applied to other problems in biology and medicine. Furthermore the dataset of cancer biomarkers is valuable to all groups curating knowledge in precision medicine and also all analysts that are interrogating the genomes of patient tumors.

Finally Chapter 6 concludes the thesis and discusses the successes and limitations of the research approaches taken. Furthermore it explores interesting future directions that could be taken with the generalized and high performing methods developed in this thesis and with the valuable precision oncology datasets extracted from literature.
